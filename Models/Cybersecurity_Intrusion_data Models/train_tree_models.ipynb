{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary imports cell to ensure kernel has required modules loaded\n",
    "import sys\n",
    "print('Python', sys.version)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Imports done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e375174",
   "metadata": {},
   "source": [
    "# Decision Tree and Random Forest training notebook\n",
    "\n",
    "This notebook builds, evaluates, visualizes, tunes, and saves Decision Tree and Random Forest models for the\n",
    "`cybersecurity_intrusion_data.csv` dataset. It follows the project's preprocessing conventions (numeric scaling + one-hot\n",
    "encoding for categoricals) and saves models to the repository `models/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Load dataset\n",
    "# Notebook-friendly path (workspace relative). Adjust if you open the notebook from a different working directory.\n",
    "DATA_PATH = Path('../../Sheets/cybersecurity_intrusion_data.csv').resolve()\n",
    "print('Loading data from:', DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Data shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb610c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Exploratory data check & preprocessing notes\n",
    "print('\\n--- Basic info ---')\n",
    "print(df.info())\n",
    "\n",
    "print('\\n--- Description (numeric) ---')\n",
    "print(df.describe().T)\n",
    "\n",
    "print('\\n--- Class distribution ---')\n",
    "print(df['attack_detected'].value_counts())\n",
    "print(df['attack_detected'].value_counts(normalize=True))\n",
    "\n",
    "# Basic missing value check\n",
    "print('\\n--- Missing values per column ---')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Define feature groups\n",
    "numeric_features = [\n",
    "    'network_packet_size',\n",
    "    'login_attempts',\n",
    "    'session_duration',\n",
    "    'ip_reputation_score',\n",
    "    'failed_logins',\n",
    "    'unusual_time_access'\n",
    "]\n",
    "categorical_features = ['protocol_type', 'encryption_used', 'browser_type']\n",
    "\n",
    "# Keep only available columns (defensive)\n",
    "available = set(df.columns)\n",
    "numeric_features = [c for c in numeric_features if c in available]\n",
    "categorical_features = [c for c in categorical_features if c in available]\n",
    "\n",
    "print('\\nNumeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39875037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Preprocessing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Build preprocessor similar to scripts/train_models.py\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor to allow extraction of feature names later\n",
    "preprocessor.fit(df[numeric_features + categorical_features])\n",
    "\n",
    "# Derive feature names after preprocessing (for feature importance plots)\n",
    "feature_names = []\n",
    "if numeric_features:\n",
    "    feature_names.extend(numeric_features)\n",
    "if categorical_features:\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "    feature_names.extend(cat_names)\n",
    "\n",
    "print('\\nPreprocessed feature count:', len(feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2608c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Train / test split\n",
    "TARGET = 'attack_detected'\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n",
    "print('\\nTrain class balance:')\n",
    "print(y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f615d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Decision Tree — train and evaluate\n",
    "# Build pipeline combining the preprocessor and the tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[('preproc', preprocessor), ('clf', DecisionTreeClassifier(random_state=RANDOM_STATE))])\n",
    "\n",
    "# Fit\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = dt_pipeline.predict(X_test)\n",
    "print('\\nDecision Tree — Test set results')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Cross-validated ROC AUC (pipeline will apply preprocessing inside CV)\n",
    "cv_scores = cross_val_score(dt_pipeline, X, y, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "print('\\nCross-validated ROC AUC (5-fold):')\n",
    "print(cv_scores)\n",
    "print('Mean ROC AUC:', np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd21f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Decision Tree — visualize tree (shallow depth for readability)\n",
    "import matplotlib.pyplot as plt\n",
    "clf = dt_pipeline.named_steps['clf']\n",
    "\n",
    "# Transform a small sample to get the correct shape\n",
    "X_test_trans = dt_pipeline.named_steps['preproc'].transform(X_test)\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plot_tree(clf, max_depth=3, filled=True)\n",
    "plt.title('Decision Tree (top levels)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9649b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Decision Tree — feature importance & pruning\n",
    "# Feature importances (note: these correspond to the preprocessed feature ordering)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "imp = clf.feature_importances_\n",
    "if len(imp) == len(feature_names):\n",
    "    fi = pd.Series(imp, index=feature_names).sort_values(ascending=False)\n",
    "else:\n",
    "    fi = pd.Series(imp).sort_values(ascending=False)\n",
    "\n",
    "print('\\nTop features (Decision Tree):')\n",
    "print(fi.head(15))\n",
    "\n",
    "# Cost-complexity pruning path\n",
    "X_train_trans = dt_pipeline.named_steps['preproc'].transform(X_train)\n",
    "path = clf.cost_complexity_pruning_path(X_train_trans, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Train trees using the different alphas\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas[:10]:  # limit for speed\n",
    "    clf_tmp = DecisionTreeClassifier(random_state=RANDOM_STATE, ccp_alpha=ccp_alpha)\n",
    "    clf_tmp.fit(X_train_trans, y_train)\n",
    "    clfs.append((ccp_alpha, clf_tmp))\n",
    "\n",
    "print('\\nTrained {} pruned trees (sample of alphas)'.format(len(clfs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Random Forest — train and evaluate\n",
    "rf_pipeline = Pipeline(steps=[('preproc', preprocessor), ('clf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, oob_score=True, n_jobs=-1))])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "print('\\nRandom Forest — Test set results')\n",
    "print(classification_report(y_test, y_pred_rf, digits=4))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# OOB score\n",
    "rf_clf = rf_pipeline.named_steps['clf']\n",
    "if hasattr(rf_clf, 'oob_score_'):\n",
    "    print('\\nRandom Forest OOB score:', rf_clf.oob_score_)\n",
    "\n",
    "# Feature importances\n",
    "imp_rf = rf_clf.feature_importances_\n",
    "if len(imp_rf) == len(feature_names):\n",
    "    fi_rf = pd.Series(imp_rf, index=feature_names).sort_values(ascending=False)\n",
    "else:\n",
    "    fi_rf = pd.Series(imp_rf).sort_values(ascending=False)\n",
    "\n",
    "print('\\nTop features (Random Forest):')\n",
    "print(fi_rf.head(15))\n",
    "\n",
    "# Permutation importance (optional, can be slower)\n",
    "perm = permutation_importance(rf_pipeline, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "perm_idx = perm.importances_mean.argsort()[::-1]\n",
    "print('\\nTop permutation importances:')\n",
    "for i in perm_idx[:10]:\n",
    "    name = feature_names[i] if i < len(feature_names) else f'X{i}'\n",
    "    print(f\"{name}: {perm.importances_mean[i]:.4f} (+/- {perm.importances_std[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Hyperparameter tuning (GridSearchCV example for Random Forest)\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__max_depth': [None, 10, 20],\n",
    "    'clf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(rf_pipeline, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nBest params (GridSearch):', grid.best_params_)\n",
    "print('Best CV ROC AUC:', grid.best_score_)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "# Evaluate on test set\n",
    "y_pred_grid = best_rf.predict(X_test)\n",
    "print('\\nGrid Seached RF — Test set')\n",
    "print(classification_report(y_test, y_pred_grid, digits=4))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab75f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Save models and notebook export notes\n",
    "MODEL_DIR = Path('../../models').resolve()\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(dt_pipeline, MODEL_DIR / 'decision_tree_model.joblib')\n",
    "joblib.dump(rf_pipeline, MODEL_DIR / 'random_forest_model.joblib')\n",
    "# Save the best grid-searched model if available\n",
    "try:\n",
    "    joblib.dump(best_rf, MODEL_DIR / 'random_forest_best_grid.joblib')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print('Models saved to:', MODEL_DIR)\n",
    "\n",
    "# Notebook export note (run in integrated terminal if you want a clean exported copy):\n",
    "# jupyter nbconvert --to notebook --output exported_train_tree_models.ipynb train_tree_models.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
