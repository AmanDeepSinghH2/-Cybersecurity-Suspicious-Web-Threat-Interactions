{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdf8855",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering Analysis: Global Cybersecurity Threats\n",
    "\n",
    "This notebook performs hierarchical clustering analysis on the Global Cybersecurity Threats dataset (2015-2024) to identify hierarchical patterns and relationships in cybersecurity incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bd2da",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our hierarchical clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f6a9f",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "Let's load our Global Cybersecurity Threats dataset and prepare it for hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../Sheets/Global_Cybersecurity_Threats_2015-2024.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFeature Information:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6de92d",
   "metadata": {},
   "source": [
    "## 3. Preprocess Features\n",
    "\n",
    "Let's prepare our data for hierarchical clustering by:\n",
    "1. Encoding categorical variables\n",
    "2. Scaling numerical features\n",
    "3. Creating the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd533587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Initialize label encoders for categorical columns\n",
    "categorical_columns = ['Country', 'Attack Type', 'Target Industry', \n",
    "                      'Attack Source', 'Security Vulnerability Type', \n",
    "                      'Defense Mechanism Used']\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical variables\n",
    "for column in categorical_columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "\n",
    "# Select features for clustering\n",
    "features_for_clustering = [\n",
    "    'Year',\n",
    "    'Country',\n",
    "    'Attack Type',\n",
    "    'Target Industry',\n",
    "    'Financial Loss (in Million $)',\n",
    "    'Number of Affected Users',\n",
    "    'Attack Source',\n",
    "    'Security Vulnerability Type',\n",
    "    'Defense Mechanism Used',\n",
    "    'Incident Resolution Time (in Hours)'\n",
    "]\n",
    "\n",
    "# Create feature matrix X\n",
    "X = df_processed[features_for_clustering].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Scaled feature matrix shape:\", X_scaled.shape)\n",
    "print(\"\\nFeature names:\", features_for_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc7988",
   "metadata": {},
   "source": [
    "## 4. Compute Linkage Matrix and Create Dendrogram\n",
    "\n",
    "Now we'll perform hierarchical clustering using different linkage methods and visualize the results using dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fe06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the linkage matrix using different methods\n",
    "methods = ['ward', 'complete', 'average', 'single']\n",
    "linkage_matrices = {}\n",
    "\n",
    "for method in methods:\n",
    "    linkage_matrices[method] = hierarchy.linkage(X_scaled, method=method)\n",
    "\n",
    "# Create a figure with subplots for different linkage methods\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, method in enumerate(methods, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    \n",
    "    # Create dendrogram\n",
    "    hierarchy.dendrogram(\n",
    "        linkage_matrices[method],\n",
    "        truncate_mode='lastp',  # show only the last p merged clusters\n",
    "        p=30,  # show only the last 30 merged clusters\n",
    "        show_leaf_counts=True,\n",
    "        leaf_rotation=90.,\n",
    "        leaf_font_size=8.,\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Hierarchical Clustering Dendrogram\\n(method: {method})')\n",
    "    plt.xlabel('Sample index or (cluster size)')\n",
    "    plt.ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute and print the cophenetic correlation coefficient for each method\n",
    "print(\"\\nCophenetic Correlation Coefficient for each method:\")\n",
    "for method in methods:\n",
    "    c, _ = hierarchy.cophenet(linkage_matrices[method], distance.pdist(X_scaled))\n",
    "    print(f\"{method}: {c:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46beb9a4",
   "metadata": {},
   "source": [
    "## 5. Determine Optimal Number of Clusters\n",
    "\n",
    "Let's determine the optimal number of clusters using the elbow method and silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9790908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the 'ward' method for determining optimal clusters\n",
    "# since it tends to create more balanced clusters\n",
    "linkage_matrix = linkage_matrices['ward']\n",
    "\n",
    "# Calculate distances for different numbers of clusters\n",
    "max_clusters = 10\n",
    "distances = []\n",
    "silhouette_scores = []\n",
    "k_values = range(2, max_clusters + 1)\n",
    "\n",
    "for k in k_values:\n",
    "    # Get cluster labels\n",
    "    labels = hierarchy.fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    \n",
    "    # Calculate total distance\n",
    "    distances.append(linkage_matrix[-(k-1), 2])\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Elbow curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, distances, 'bx-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "# Silhouette scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores, 'rx-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print silhouette scores\n",
    "print(\"\\nSilhouette Scores for different numbers of clusters:\")\n",
    "for k, score in zip(k_values, silhouette_scores):\n",
    "    print(f\"k={k}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86099078",
   "metadata": {},
   "source": [
    "## 6. Create Final Clusters and Analyze Results\n",
    "\n",
    "Based on the elbow method and silhouette analysis, let's create the final clusters and analyze their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final clusters using the optimal number of clusters (k=4 based on analysis)\n",
    "optimal_k = 4\n",
    "cluster_labels = hierarchy.fcluster(linkage_matrix, optimal_k, criterion='maxclust')\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_processed['Cluster'] = cluster_labels\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "cluster_stats = []\n",
    "\n",
    "for cluster in range(1, optimal_k + 1):\n",
    "    cluster_data = df_processed[df_processed['Cluster'] == cluster]\n",
    "    \n",
    "    # Calculate statistics for numerical features\n",
    "    financial_loss_mean = cluster_data['Financial Loss (in Million $)'].mean()\n",
    "    affected_users_mean = cluster_data['Number of Affected Users'].mean()\n",
    "    resolution_time_mean = cluster_data['Incident Resolution Time (in Hours)'].mean()\n",
    "    \n",
    "    # Get most common values for categorical features\n",
    "    most_common_attack = label_encoders['Attack Type'].inverse_transform([cluster_data['Attack Type'].mode()[0]])[0]\n",
    "    most_common_industry = label_encoders['Target Industry'].inverse_transform([cluster_data['Target Industry'].mode()[0]])[0]\n",
    "    most_common_source = label_encoders['Attack Source'].inverse_transform([cluster_data['Attack Source'].mode()[0]])[0]\n",
    "    \n",
    "    cluster_stats.append({\n",
    "        'Cluster': cluster,\n",
    "        'Size': len(cluster_data),\n",
    "        'Avg Financial Loss': financial_loss_mean,\n",
    "        'Avg Affected Users': affected_users_mean,\n",
    "        'Avg Resolution Time': resolution_time_mean,\n",
    "        'Most Common Attack': most_common_attack,\n",
    "        'Most Common Industry': most_common_industry,\n",
    "        'Most Common Source': most_common_source\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with cluster statistics\n",
    "cluster_summary = pd.DataFrame(cluster_stats)\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('Hierarchical Clustering Results (PCA Visualization)')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio: {explained_variance}\")\n",
    "print(f\"Total variance explained: {sum(explained_variance):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb2b35",
   "metadata": {},
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Let's save the clustering results and the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the hierarchical clustering model\n",
    "clustering_results = {\n",
    "    'model': hierarchical_cluster,\n",
    "    'labels': labels,\n",
    "    'scaled_data': X_scaled,\n",
    "    'feature_names': feature_names,\n",
    "    'linkage_matrix': Z\n",
    "}\n",
    "\n",
    "# Save the model and results\n",
    "joblib.dump(clustering_results, 'hierarchical_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc541de",
   "metadata": {},
   "source": [
    "## 8. Cluster Analysis and Interpretation\n",
    "\n",
    "Let's analyze the characteristics of each cluster to understand the patterns in cybersecurity threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b77db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the original dataframe\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Calculate mean values for each cluster\n",
    "cluster_means = df.groupby('Cluster').mean()\n",
    "\n",
    "# Print cluster characteristics\n",
    "print(\"Cluster Characteristics:\")\n",
    "print(\"-----------------------\")\n",
    "for cluster in range(len(cluster_means)):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    print(f\"Number of instances: {len(df[df['Cluster'] == cluster])}\")\n",
    "    print(\"\\nMean values:\")\n",
    "    for feature in feature_names:\n",
    "        print(f\"{feature}: {cluster_means.loc[cluster, feature]:.2f}\")\n",
    "    \n",
    "    print(\"\\nMost common attack types:\")\n",
    "    print(df[df['Cluster'] == cluster]['Attack_Type'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a84f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster characteristics using a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "cluster_means_normalized = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "sns.heatmap(cluster_means_normalized, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Cluster Characteristics Heatmap')\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d56897",
   "metadata": {},
   "source": [
    "## 9. Temporal Analysis\n",
    "\n",
    "Let's analyze how the clusters are distributed over time to identify any temporal patterns in cybersecurity threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b676a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal analysis plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Calculate cluster distribution over time\n",
    "temporal_distribution = df.groupby(['Year', 'Cluster']).size().unstack()\n",
    "\n",
    "# Create a stacked area plot\n",
    "temporal_distribution.plot(kind='area', stacked=True)\n",
    "plt.title('Temporal Distribution of Clusters')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate relative proportions\n",
    "proportions = temporal_distribution.div(temporal_distribution.sum(axis=1), axis=0)\n",
    "\n",
    "# Create a stacked percentage plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "proportions.plot(kind='area', stacked=True)\n",
    "plt.title('Relative Proportions of Clusters Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of Incidents')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d232db",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "The hierarchical clustering analysis has helped us identify distinct patterns in cybersecurity threats. The dendrogram and cluster analysis reveal the relationships between different types of threats and their characteristics. The temporal analysis shows how these patterns have evolved over time.\n",
    "\n",
    "Key findings:\n",
    "1. The optimal number of clusters was determined through the dendrogram analysis\n",
    "2. Each cluster represents a distinct pattern of cybersecurity threats\n",
    "3. The temporal analysis reveals how threat patterns have evolved from 2015 to 2024\n",
    "4. The heatmap shows the key characteristics that distinguish each cluster\n",
    "\n",
    "This analysis can be used to:\n",
    "- Better understand the relationships between different types of cybersecurity threats\n",
    "- Identify patterns in attack characteristics\n",
    "- Track the evolution of threat patterns over time\n",
    "- Inform security strategies based on cluster characteristics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
