{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd92b8a",
   "metadata": {},
   "source": [
    "# K-means Clustering Analysis: Global Cybersecurity Threats\n",
    "\n",
    "This notebook performs K-means clustering analysis on the Global Cybersecurity Threats dataset (2015-2024) to identify patterns and groupings in cybersecurity incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e503b5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f964f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import joblib\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Set Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fa3de",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Data\n",
    "\n",
    "Let's load our Global Cybersecurity Threats dataset and perform initial data inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../Sheets/Global_Cybersecurity_Threats_2015-2024.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFeature Information:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some initial visualizations to understand the data distribution\n",
    "\n",
    "# Plot attack types distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['Attack Type'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Attack Types')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a boxplot for financial losses by attack type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Attack Type', y='Financial Loss (in Million $)', data=df)\n",
    "plt.title('Financial Loss Distribution by Attack Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation between numerical features\n",
    "numerical_cols = ['Financial Loss (in Million $)', 'Number of Affected Users', 'Incident Resolution Time (in Hours)']\n",
    "correlation = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8948fe",
   "metadata": {},
   "source": [
    "## 3. Preprocess Features\n",
    "\n",
    "We need to prepare our data for K-means clustering by:\n",
    "1. Encoding categorical variables\n",
    "2. Scaling numerical features\n",
    "3. Handling any missing values (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Initialize label encoders for categorical columns\n",
    "categorical_columns = ['Country', 'Attack Type', 'Target Industry', \n",
    "                      'Attack Source', 'Security Vulnerability Type', \n",
    "                      'Defense Mechanism Used']\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical variables\n",
    "for column in categorical_columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df_processed[column] = label_encoders[column].fit_transform(df_processed[column])\n",
    "\n",
    "# Select features for clustering\n",
    "features_for_clustering = [\n",
    "    'Year',\n",
    "    'Country',\n",
    "    'Attack Type',\n",
    "    'Target Industry',\n",
    "    'Financial Loss (in Million $)',\n",
    "    'Number of Affected Users',\n",
    "    'Attack Source',\n",
    "    'Security Vulnerability Type',\n",
    "    'Defense Mechanism Used',\n",
    "    'Incident Resolution Time (in Hours)'\n",
    "]\n",
    "\n",
    "# Create feature matrix X\n",
    "X = df_processed[features_for_clustering].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Scaled feature matrix shape:\", X_scaled.shape)\n",
    "print(\"\\nFeature names:\", features_for_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d63e5a",
   "metadata": {},
   "source": [
    "## 4. Determine Optimal Number of Clusters\n",
    "\n",
    "We'll use the elbow method and silhouette analysis to determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inertia and silhouette scores for different values of k\n",
    "k_values = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Calculate inertia\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Inertia plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, inertias, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "\n",
    "# Silhouette score plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores, 'rx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the silhouette scores\n",
    "for k, score in zip(k_values, silhouette_scores):\n",
    "    print(f\"k={k}: Silhouette Score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c616663",
   "metadata": {},
   "source": [
    "## 5. Perform K-means Clustering\n",
    "\n",
    "Based on the elbow method and silhouette analysis, let's perform K-means clustering with the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering with the optimal number of clusters\n",
    "optimal_k = 4  # We'll set this based on the elbow method and silhouette analysis results\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_processed['Cluster'] = cluster_labels\n",
    "\n",
    "# Perform PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a scatter plot of the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('K-means Clustering Results (PCA Visualization)')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio: {explained_variance}\")\n",
    "print(f\"Total variance explained: {sum(explained_variance):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_stats = []\n",
    "\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_processed[df_processed['Cluster'] == cluster]\n",
    "    \n",
    "    # Calculate statistics for numerical features\n",
    "    financial_loss_mean = cluster_data['Financial Loss (in Million $)'].mean()\n",
    "    affected_users_mean = cluster_data['Number of Affected Users'].mean()\n",
    "    resolution_time_mean = cluster_data['Incident Resolution Time (in Hours)'].mean()\n",
    "    \n",
    "    # Get most common values for categorical features\n",
    "    most_common_attack = label_encoders['Attack Type'].inverse_transform([cluster_data['Attack Type'].mode()[0]])[0]\n",
    "    most_common_industry = label_encoders['Target Industry'].inverse_transform([cluster_data['Target Industry'].mode()[0]])[0]\n",
    "    most_common_source = label_encoders['Attack Source'].inverse_transform([cluster_data['Attack Source'].mode()[0]])[0]\n",
    "    \n",
    "    cluster_stats.append({\n",
    "        'Cluster': cluster,\n",
    "        'Size': len(cluster_data),\n",
    "        'Avg Financial Loss': financial_loss_mean,\n",
    "        'Avg Affected Users': affected_users_mean,\n",
    "        'Avg Resolution Time': resolution_time_mean,\n",
    "        'Most Common Attack': most_common_attack,\n",
    "        'Most Common Industry': most_common_industry,\n",
    "        'Most Common Source': most_common_source\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with cluster statistics\n",
    "cluster_summary = pd.DataFrame(cluster_stats)\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c9634",
   "metadata": {},
   "source": [
    "## 6. Save the Model\n",
    "\n",
    "Let's save the trained K-means model and the preprocessing objects for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and preprocessing objects\n",
    "model_data = {\n",
    "    'kmeans_model': kmeans,\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders,\n",
    "    'pca': pca,\n",
    "    'feature_names': features_for_clustering\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(model_data, 'cybersecurity_kmeans_model.joblib')\n",
    "print(\"Model and preprocessing objects saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
