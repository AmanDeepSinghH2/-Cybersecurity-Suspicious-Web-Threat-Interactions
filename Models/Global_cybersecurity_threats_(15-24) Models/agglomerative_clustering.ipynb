{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fa646a",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering Analysis of Cybersecurity Threats\n",
    "\n",
    "This notebook implements Agglomerative Clustering to analyze patterns in cybersecurity threats from 2015 to 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca698a0f",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "First, let's load and explore our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f48c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../Sheets/Global_Cybersecurity_Threats_2015-2024.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.info())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(\"-\" * 40)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22371c",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Let's prepare our data for clustering by encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Create LabelEncoder for categorical columns\n",
    "le = LabelEncoder()\n",
    "df['Attack_Type'] = le.fit_transform(df['Attack_Type'])\n",
    "\n",
    "# Select features for clustering\n",
    "feature_columns = ['Attack_Type', 'Financial_Loss', 'User_Impact', 'Year']\n",
    "X = df[feature_columns].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Store feature names for later use\n",
    "feature_names = feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08963fea",
   "metadata": {},
   "source": [
    "## 3. Linkage Analysis\n",
    "\n",
    "We'll analyze different linkage methods to determine the best approach for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different linkage methods\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for idx, method in enumerate(linkage_methods, 1):\n",
    "    # Compute linkage matrix\n",
    "    Z = linkage(X_scaled, method=method)\n",
    "    \n",
    "    # Create subplot\n",
    "    plt.subplot(2, 2, idx)\n",
    "    dendrogram(Z)\n",
    "    plt.title(f'Dendrogram using {method} linkage')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ecce2",
   "metadata": {},
   "source": [
    "## 4. Determine Optimal Number of Clusters\n",
    "\n",
    "Let's use the silhouette score to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of clusters\n",
    "n_clusters_range = range(2, 11)\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in n_clusters_range:\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {n_clusters}, silhouette score = {silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters_range, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Get optimal number of clusters\n",
    "optimal_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters: {optimal_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500c43e",
   "metadata": {},
   "source": [
    "## 5. Perform Agglomerative Clustering\n",
    "\n",
    "Now we'll perform the clustering with the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Agglomerative Clustering with optimal number of clusters\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=optimal_clusters)\n",
    "cluster_labels = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "# Print cluster sizes\n",
    "print(\"Cluster Sizes:\")\n",
    "print(\"-\" * 40)\n",
    "print(df['Cluster'].value_counts().sort_index())\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = []\n",
    "for i in range(optimal_clusters):\n",
    "    cluster_center = X_scaled[cluster_labels == i].mean(axis=0)\n",
    "    cluster_centers.append(cluster_center)\n",
    "cluster_centers = np.array(cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2094a7d",
   "metadata": {},
   "source": [
    "## 6. Cluster Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster visualization using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('Agglomerative Clustering Results (PCA Visualization)')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display explained variance ratio\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio: {explained_var_ratio}\")\n",
    "print(f\"Total explained variance: {sum(explained_var_ratio):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc457a03",
   "metadata": {},
   "source": [
    "## 7. Cluster Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342539f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_stats = df.groupby('Cluster').agg({\n",
    "    'Attack_Type': ['count', 'mean'],\n",
    "    'Financial_Loss': ['mean', 'std'],\n",
    "    'User_Impact': ['mean', 'std'],\n",
    "    'Year': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Cluster Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "print(cluster_stats)\n",
    "\n",
    "# Create heatmap of cluster characteristics\n",
    "plt.figure(figsize=(12, 8))\n",
    "cluster_means = df.groupby('Cluster')[feature_columns].mean()\n",
    "cluster_means_scaled = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "sns.heatmap(cluster_means_scaled, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Cluster Characteristics Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7783f",
   "metadata": {},
   "source": [
    "## 8. Temporal Analysis of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99588705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal distribution of clusters\n",
    "temporal_dist = pd.crosstab(df['Year'], df['Cluster'])\n",
    "\n",
    "# Plot temporal distribution\n",
    "plt.figure(figsize=(15, 8))\n",
    "temporal_dist.plot(kind='bar', stacked=True)\n",
    "plt.title('Distribution of Clusters Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot relative proportions\n",
    "plt.figure(figsize=(15, 8))\n",
    "temporal_dist_pct = temporal_dist.div(temporal_dist.sum(axis=1), axis=0)\n",
    "temporal_dist_pct.plot(kind='bar', stacked=True)\n",
    "plt.title('Relative Proportions of Clusters Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Proportion of Incidents')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fd78d",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e857f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save clustering results\n",
    "clustering_results = {\n",
    "    'model': agg_clustering,\n",
    "    'labels': cluster_labels,\n",
    "    'scaled_data': X_scaled,\n",
    "    'feature_names': feature_names,\n",
    "    'optimal_clusters': optimal_clusters,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': le\n",
    "}\n",
    "\n",
    "# Save the model and results\n",
    "joblib.dump(clustering_results, 'agglomerative_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd2a2f",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "The Agglomerative Clustering analysis has revealed distinct patterns in cybersecurity threats:\n",
    "\n",
    "1. We identified the optimal number of clusters through silhouette analysis\n",
    "2. Each cluster represents a unique pattern of threat characteristics\n",
    "3. The temporal analysis shows how these patterns have evolved over time\n",
    "4. The cluster characteristics provide insights into different types of cybersecurity threats\n",
    "\n",
    "These insights can be used to:\n",
    "- Better understand the relationships between different types of cybersecurity threats\n",
    "- Identify common patterns in attack characteristics\n",
    "- Track the evolution of threat patterns over time\n",
    "- Inform security strategies based on cluster characteristics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
